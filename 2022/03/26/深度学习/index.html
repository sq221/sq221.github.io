<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.8.2","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="深度学习入门——用python从轮子开始造神经网络前言由于作者水平问题，对深度学习的原理也给不出什么独到精妙的理解，本文单从实践角度试图让深度学习看起来简单一点。 本文不涉及任何已有框架如pytorch、tensorflow，环境为anaconda的默认base环境。 本文使用数据集：MNIST数据集 需要提前指出的是，依据本文内容写出的神经网络是最最简单最最原始的那种，换言之，你肯定没法跑出结果">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习.md">
<meta property="og:url" content="http://example.com/2022/03/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="时棋的自留地">
<meta property="og:description" content="深度学习入门——用python从轮子开始造神经网络前言由于作者水平问题，对深度学习的原理也给不出什么独到精妙的理解，本文单从实践角度试图让深度学习看起来简单一点。 本文不涉及任何已有框架如pytorch、tensorflow，环境为anaconda的默认base环境。 本文使用数据集：MNIST数据集 需要提前指出的是，依据本文内容写出的神经网络是最最简单最最原始的那种，换言之，你肯定没法跑出结果">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-03-26T02:17:31.000Z">
<meta property="article:modified_time" content="2022-03-26T02:23:58.295Z">
<meta property="article:author" content="时棋">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2022/03/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2022/03/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","path":"2022/03/26/深度学习/","title":"深度学习.md"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>深度学习.md | 时棋的自留地</title>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?99c73a51e708ba202adb181cc7e8cdaa""></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">时棋的自留地</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E2%80%94%E2%80%94%E7%94%A8python%E4%BB%8E%E8%BD%AE%E5%AD%90%E5%BC%80%E5%A7%8B%E9%80%A0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">深度学习入门——用python从轮子开始造神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MNIST%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.2.</span> <span class="nav-text">MNIST数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.3.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.3.1.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.3.2.</span> <span class="nav-text">损失函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.4.</span> <span class="nav-text">神经网络的学习</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="时棋"
      src="/images/17.gif">
  <p class="site-author-name" itemprop="name">时棋</p>
  <div class="site-description" itemprop="description">但我将坦然，欣然。我将大笑，我将歌唱。</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/17.gif">
      <meta itemprop="name" content="时棋">
      <meta itemprop="description" content="但我将坦然，欣然。我将大笑，我将歌唱。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="时棋的自留地">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习.md
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-03-26 10:17:31 / 修改时间：10:23:58" itemprop="dateCreated datePublished" datetime="2022-03-26T10:17:31+08:00">2022-03-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%BC%96%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">编程</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="深度学习入门——用python从轮子开始造神经网络"><a href="#深度学习入门——用python从轮子开始造神经网络" class="headerlink" title="深度学习入门——用python从轮子开始造神经网络"></a>深度学习入门——用python从轮子开始造神经网络</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于作者水平问题，对深度学习的原理也给不出什么独到精妙的理解，本文单从实践角度试图让深度学习看起来简单一点。</p>
<p>本文不涉及任何已有框架如pytorch、tensorflow，环境为anaconda的默认base环境。</p>
<p>本文使用数据集：<a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">MNIST数据集</a></p>
<p>需要提前指出的是，依据本文内容写出的神经网络是最最简单最最原始的那种，换言之，你肯定没法跑出结果，因为算法效率太低了，同时不支持借用GPU算力。如何提升效率？请期待后续更新……</p>
 <span id="more"></span>
<p>我们从图像分类问题开始。众所周知，计算机以数组形式阅读图片，简而言之，我们的程序要求输入一个数组，最后输出对它的分类，也可以用数组表示。比如，输入数组$(x_1,x_2,x_3)$，要确定它是三类中的哪一类，若是第二类，则输出应为$(0,1,0)$</p>
<h2 id="MNIST数据集"><a href="#MNIST数据集" class="headerlink" title="MNIST数据集"></a>MNIST数据集</h2><p>这里介绍一下我们使用的数据集：MNIST</p>
<p>MNIST数据集(Mixed National Institute of Standards and Technology database)是美国国家标准与技术研究院收集整理的大型手写数字数据库,包含60,000个示例的训练集以及10,000个示例的测试集。</p>
<p>在官方网站上有4个gz压缩文件，包含训练集与测试集，其中又分为图像集与标签集。每个图像是一张28 <em> 28像素的灰度手写数字图片。简而言之，我们读取的图片为1 </em> 28 <em> 28的数组，整个训练集的图片则是60000 </em> 28 <em> 28的数组，标签集则是1 </em> 60000的数组，每个元素对应一张图片，比如第三张图片是5，那么标签数组中的第三个元素就是5。</p>
<p>读取训练集的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_mnist_train</span>(<span class="params">path</span>):</span></span><br><span class="line">    <span class="comment">#path为你存放gz文件的目录路径</span></span><br><span class="line">    labels_path = os.path.join(path,<span class="string">&#x27;train-labels-idx1-ubyte.gz&#x27;</span>)<span class="comment">#标签集路径</span></span><br><span class="line">    images_path = os.path.join(path,<span class="string">&#x27;train-images-idx3-ubyte.gz&#x27;</span>)<span class="comment">#图像集路径    </span></span><br><span class="line">    <span class="keyword">with</span> gzip.<span class="built_in">open</span>(labels_path,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> lbpath:</span><br><span class="line">        magic , number = struct.unpack(<span class="string">&#x27;&gt;II&#x27;</span>,lbpath.read(<span class="number">8</span>))<span class="comment">#去掉开头我们不需要的数据</span></span><br><span class="line">        label = np.fromstring(lbpath.read(),dtype = np.uint8)<span class="comment">#读取为np数组</span></span><br><span class="line">    <span class="keyword">with</span> gzip.<span class="built_in">open</span>(images_path,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> impath:</span><br><span class="line">        magic , num , rows , cols = struct.unpack(<span class="string">&#x27;&gt;IIII&#x27;</span>,impath.read(<span class="number">16</span>))</span><br><span class="line">        images = np.fromstring(impath.read(),dtype = np.uint8).reshape(<span class="built_in">len</span>(label),<span class="number">784</span>)</span><br><span class="line">        <span class="comment">#将原为60000*28*28的数组转化为60000*784的数组</span></span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> images,labels</span><br></pre></td></tr></table></figure>
<p>读取测试集同理。</p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>之后举例就就近拿MNIST数据集举例了，也方便实践。</p>
<p>现在我们的需求是，训练出一个能够正确对图像数据进行分类的神经网络。那么，什么是神经网络？</p>
<p>不涉及具体技术细节，我们也可以描述其大概的结构。首先神经网络要有输入和输出，输入的是图像数据，输出对其的分类预测值，中间经过一系列数据处理……没错就这三层，输入层、中间层、输出层。中间层也称为隐藏层，神经网络的深度就是指中间层的层数。本文目标是从头造一个最简单的神经网络，那就只造一层，同时忽略偏置。</p>
<p>以MNIST数据集为例，一张图片输入进神经网络，输入层就是1 * 784的数组，也就是说有784个参数。中间层可以设置任意个数的神经元，接收输入层数据后经过一定处理将数据传给输出层，由于我们这里造的是单层神经网络，所以中间层和输出层其实合并了，直观上看就是在中间层完成了所有处理。</p>
<p>宏观上大致扫一遍，下面从具体细节看，每个神经元都要接收所有输入的数据，然后依据一个函数对其进行处理再输出。显然我们不可能只是把所有输入值相加，这里就涉及到神经网络可以称得上智能的地方了。输入值是按一定权重被神经元接收的，比如，2个神经元接收3个数据，就可以表示成：</p>
<script type="math/tex; mode=display">
\begin{cases}a_1=w_{11}x_1+w_{21}x_2+w_{31}x_3\\a_2=w_{12}x_1+w_{22}x_2+w_{32}x_3\end{cases}</script><p>其中$a_1,a_2$即为神经元接收到的值，$x_1,x_2,x_3$即为输入值，$w$为各输入值的权重。理论上，每个式子后面都可以分别加上一个常数$b$作为偏置，但为求简略这里就忽略了。用矩阵的写法来看，那就是：</p>
<script type="math/tex; mode=display">
A=\begin{pmatrix}x_1&x_2&x_3\end{pmatrix}\begin{pmatrix}w_{11}&w_{12}\\w_{21}&w_{22}\\w_{31}&w_{32}\end{pmatrix}=\begin{pmatrix}a_1&a_2\end{pmatrix}</script><p>这里的权重矩阵就是神经网络能够对图像进行分类的核心所在，在机器学习中，这个权重矩阵是由人工确定的，而在深度学习中，这个矩阵是可以通过训练得到的，不涉及人工。</p>
<p>初始化生成一个权重矩阵的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(input_layel,output_layel)</span><br><span class="line"><span class="comment">#第一个参数是输入层的数据个数，在单层网络的情况下第二个参数为输出层的参数个数，亦即要分类的类别个数</span></span><br></pre></td></tr></table></figure>
<p>numpy中的矩阵乘法函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A = np.dot(x,w)</span><br><span class="line"><span class="comment">#注意矩阵形状正确</span></span><br></pre></td></tr></table></figure>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>现在中间层接收到的矩阵$A$相当于未经内部处理的“刺激”，还需要神经元内部的函数对其进行转化，这个函数被称为激活函数。在感知机中，这个激活函数一般为阶跃函数，即：</p>
<script type="math/tex; mode=display">
f(x)=\begin{cases}1& x\ge0\\-1& x<0\end{cases}</script><p>这个函数既不连续又不可导，差评，换人。</p>
<p>sigmoid函数：</p>
<script type="math/tex; mode=display">
f(x)=\frac{1}{1+e^{-x}}</script><p>ReLU函数：</p>
<script type="math/tex; mode=display">
f(x)=\begin{cases}x&x>0\\0&x\le0\end{cases}</script><p>这两个是常用的激活函数，下面我们会看到为什么激活函数需要连续可导，这是梯度下降算法的关键。</p>
<p>实现这两个函数的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_function</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReLU_function</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(x,<span class="number">0</span>)</span><br><span class="line"><span class="comment">#参数均为np数组</span></span><br></pre></td></tr></table></figure>
<p>输出层需要输出具有概率意义的数据，即在0，1之间的数据，作为分类的预测值，二元分类输出层的激活函数一般为sigmoid函数，多元分类为softmax函数。</p>
<p>softmax函数：</p>
<script type="math/tex; mode=display">
y_k=\frac{e^{x_k}}{\sum\limits_{i=1}\limits^ne^{x_i}}</script><p>这里的$y_k$即为图像为第k类的概率的预测值。</p>
<p>但由于指数运算容易超出程序可处理的范围，所以要对函数做一些改进，原理如下：</p>
<script type="math/tex; mode=display">
\begin{align}
y_k&=\frac{e^{x_k}}{\sum\limits_{i=1}\limits^ne^{x_i}}=\frac{Ce^{x_k}}{C\sum\limits_{i=1}\limits^ne^{x_i}}=\frac{e^{x_k+logC}}{\sum\limits_{i=1}\limits^ne^{x_i+logC}}\\&=\frac{e^{x_k+C'}}{\sum\limits_{i=1}\limits^ne^{x_i+C'}}
\end{align}</script><p>简而言之，可以对输入的数组每个元素都加一个常数。一般来说，我们减去数组中的最大值。</p>
<p>实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_function</span>(<span class="params">x</span>):</span></span><br><span class="line">    c = np.<span class="built_in">max</span>(x)</span><br><span class="line">    exp_x = np.exp(x-c)</span><br><span class="line">    y = exp_x/np.<span class="built_in">sum</span>(exp_x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<p>最后输出的就是最终的预测值了，以MNIST数据集为例，输出的数组中含有10个元素，每个元素对应其属于对应类的概率，如输入图像为5，那么我们预期中神经网络的输出数组的第五个元素应该显著大于其余元素。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>那么如何描述这个显著呢？引入损失函数，即预测值与实际值的偏差。常见的损失函数有均方误差与交叉熵误差。</p>
<p>均方误差：</p>
<script type="math/tex; mode=display">
E=\frac{\sum(y_k-t_k)^2}{k}</script><p>k为数组中数据的个数，$y_k$表示有k个元素的预测值，$t_k$为实际值，其中核心是分子的式子，不改变核心的前提下可以对其进行放缩，不妨将其放大$\frac{k}{2}$倍，变成：</p>
<script type="math/tex; mode=display">
E=\frac{1}{2}\sum(y_k-t_k)^2</script><p>实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_squared_error</span>(<span class="params">y,t</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span>*np.<span class="built_in">sum</span>((y-t)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>交叉熵误差：</p>
<script type="math/tex; mode=display">
E=-\sum t_k\log y_k</script><p>由于实际值数组中除正确分类对应位置的元素为1，其余都为0，实际上交叉熵误差的值就是负的预测值对应位置元素的自然对数。</p>
<p>为了防止运算中$y_k$内有元素为0导致的错误情况，对每个元素加上一个小值delta。</p>
<p>实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span>(<span class="params">y,t</span>):</span></span><br><span class="line">    delta = <span class="number">1e-7</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t*np.log(y+delta))</span><br></pre></td></tr></table></figure>
<p>以上损失函数的实现实际上对我们的实际值数组有要求，必须写成我最开始举例的那个形式，如三类中的第二类$(0,1,0)$（称为one-hot表示），而我们先前读取的MNIST数据集很遗憾并非初始如此，必须经过手动调整。方式很多，此处放下不提。</p>
<p>总体来看，我们的神经网络本身具备的性质为：神经元个数、中间层激活函数、输出层激活函数、损失函数。针对不同问题，需要输入的参数是：输入层参数个数、输出层参数个数、权重矩阵、训练集。</p>
<h2 id="神经网络的学习"><a href="#神经网络的学习" class="headerlink" title="神经网络的学习"></a>神经网络的学习</h2><p>以上按前向顺序描述了神经网络的运行流程，接下来是学习过程，也就是根据训练集找到最优的权重矩阵。我们可以数学化地将其描述为，求损失函数（因变量）值最小时的w（自变量）</p>
<p>学过高中数学的都知道，求最小值或极小值要求导。在多元函数的情况下，就是求偏导。现有的框架大都自带求导函数，但我们现在没有轮子……所以还得从头开始。</p>
<p>显然我们没法求出每个函数微分的解析解，我们采用数值法，也就是依据导数定义……最简单的一元函数求导数的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numerical_diff</span>(<span class="params">f,x</span>):</span></span><br><span class="line">    h = <span class="number">1e-4</span></span><br><span class="line">    fx1 = f(x+h)</span><br><span class="line">    fx2 = f(x-h)</span><br><span class="line">    <span class="keyword">return</span> (fx1+fx2)/2h</span><br></pre></td></tr></table></figure>
<p>此为函数f在x点处的导数。若要求多元函数的导数并对每个变量求导的值放在一个数组里，函数会复杂一点，最终得到的数组即为梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numerical_gradient</span>(<span class="params">f, x</span>):</span></span><br><span class="line">    <span class="comment">#这里输入的x即为我们的权重矩阵，为二维数组</span></span><br><span class="line">    h = <span class="number">1e-4</span> <span class="comment"># 0.0001</span></span><br><span class="line">    grad = np.zeros_like(x) <span class="comment"># 生成和x形状相同的数组</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> idy <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">1</span>]):</span><br><span class="line">            tmp_val = x[idx][idy]</span><br><span class="line">            <span class="comment"># f(x+h)的计算</span></span><br><span class="line">            x[idx][idy] = tmp_val + h</span><br><span class="line">            fxh1 = f(x)</span><br><span class="line">            <span class="comment"># f(x-h)的计算</span></span><br><span class="line">            x[idx][idy] = tmp_val - h</span><br><span class="line">            fxh2 = f(x)</span><br><span class="line">            grad[idx][idy] = (fxh1 - fxh2) / (<span class="number">2</span>*h)</span><br><span class="line">            x[idx][idy] = tmp_val <span class="comment"># 还原值</span></span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<p>接下来就是求极小值的过程了，也即为梯度下降法。具体的流程简单来说就是依据梯度中对应的导数值令相应变量向对应方向移动一步，由数学原理可知应当离极小值点更近，再次计算梯度，重复以上流程。其中需要注意的参数是，每次移动的步幅以及重复次数。</p>
<p>步幅对应的参数为学习率（learning-rate)，过小过大都不合适，这个需要多次尝试以找到最优解。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">f, init_x, lr=<span class="number">0.01</span>, step_num=<span class="number">10</span></span>):</span> </span><br><span class="line">    <span class="comment">#参数依次为：目标函数、初始值、学习率、重复次数</span></span><br><span class="line">    x = init_x </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(step_num):</span><br><span class="line">        grad = numerical_gradient(f, x)</span><br><span class="line">        x -= lr * grad</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>到这儿，所有准备工作差不多都完成了。当然，训练过程中我们不可能直接拿有60000个数据的训练集整体来学习，而是从中抽取较小的一簇数据，这一步就略过不提。</p>
<p>下面来搭建神经网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">simpleNet</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_layel,output_layel,batch,x,t</span>):</span></span><br><span class="line">        <span class="comment">#输入参数个数、输出参数个数、学习的一簇数据个数、图像集、标签集</span></span><br><span class="line">        self.input_layel = input_layel</span><br><span class="line">        self.output_layel = output_layel</span><br><span class="line">        self.batch = batch</span><br><span class="line">        self.x = x</span><br><span class="line">        self.t = t</span><br><span class="line">        self.w = np.random.randn(input_layel,output_layel)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self,w</span>):</span></span><br><span class="line">        <span class="comment">#输入权重矩阵</span></span><br><span class="line">        step_1 = np.dot(self.x,w)</span><br><span class="line">        step_2 = ReLU_function(step_1) <span class="comment">#采用ReLU函数作为激活函数</span></span><br><span class="line">        <span class="keyword">return</span> softmax_function(step_2) <span class="comment">#假装自己还有一层输出层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self,w</span>):</span></span><br><span class="line">        n = self.batch</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.batch):</span><br><span class="line">            loss += cross_entropy_error(self.predict(w)[i],self.t)</span><br><span class="line">        loss = loss/n <span class="comment">#计算损失函数的均值</span></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>开始攻击MNIST数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_batch = get_batch_100(images,labels)</span><br><span class="line"><span class="comment">#写一个函数来随机抽取训练集，这里抽100个</span></span><br><span class="line">im_batch = train_batch[<span class="number">0</span>]</span><br><span class="line">lb_batch = train_batch[<span class="number">1</span>]</span><br><span class="line">My_first_net = simpleNet(<span class="number">784</span>,<span class="number">10</span>,<span class="number">100</span>,im_batch,lb_batch)</span><br><span class="line">w = My_first_net.w</span><br><span class="line">w = gradient_descent(My_first_net.loss,w)</span><br></pre></td></tr></table></figure>
<p>这样应该就能获得一个对应的权重矩阵来分类了（理论上）</p>
<p>当然一开始就说过，算法过于原始，实际上是跑不出来的。。。</p>
<p>你已经掌握了神经网络最基本的原理了，接下来要做的就是提高效率。</p>
<p>To be continued……</p>
<p>参考书：《深度学习入门：基于Python的理论与实现》 by 斋藤康毅 </p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="followme">
  <span>欢迎关注我的其它发布渠道</span>

  <div class="social-list">

      <div class="social-item">
        <a target="_blank" class="social-link" href="/uploads/wechat.jpg">
          <span class="icon">
            <i class="fab fa-weixin"></i>
          </span>

          <span class="label">WeChat</span>
        </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/01/26/python%E7%AC%94%E8%AE%B0/" rel="prev" title="python笔记">
                  <i class="fa fa-chevron-left"></i> python笔记
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">时棋</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
